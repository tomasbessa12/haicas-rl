{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d412b30e",
   "metadata": {},
   "source": [
    "# <font color='deeppink'><b> Actor-Critic in Reinforcement Learning </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "import time \n",
    "import numpy as np\n",
    "import circuit.ngspice as ng\n",
    "import torchplot as tp\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from circuit.fcas import FoldedCascodeRLEnvDiscrete\n",
    "import math\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Plots \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import logging\n",
    "# Setting a fixed seed for reproducible results\n",
    "np.random.seed(seed=12)\n",
    "torch.manual_seed(seed=12)\n",
    "\n",
    "\n",
    "#import pygame\n",
    "#from pygame import gfxdraw\n",
    "\n",
    "\n",
    "log_file = 'fcas.log'\n",
    "with open(log_file, \"w\") as f:\n",
    "    f.write(str(datetime.now().time()))\n",
    "\n",
    "logging.basicConfig(filename=log_file, level=logging.DEBUG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60469542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = FoldedCascodeRLEnvDiscrete() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721906e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66defdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} actions\".format(env.action_size))\n",
    "print(env.reset().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f59ec",
   "metadata": {},
   "source": [
    "# <font color='orangered'><b> Model Network </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5023d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network: Actor-Critic \n",
    "#     \n",
    "# First fully connected layer with 23 inputs (states) and 256 outputs (arbitrary number)\n",
    "# \n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.common1 = torch.nn.Linear(30, 1024) # x because there are x parameters as the observation space\n",
    "        self.common2 = torch.nn.Linear(1024, 512)\n",
    "        \n",
    "        self.actor = torch.nn.Linear(512, 62) # 64 for the number of actions\n",
    "\n",
    "        # self.fc3 = torch.nn.Linear(128,20)\n",
    "        self.critic = torch.nn.Linear(512, 1) # Critic is always 1\n",
    "        \n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "                #print(f\"{x= }\")\n",
    "        #print(f\"{action_scores= }\")\n",
    "\n",
    "        x = F.leaky_relu(self.common1(x))\n",
    "        x = F.leaky_relu(self.common2(x))        \n",
    "\n",
    "        action_scores = torch.sigmoid(self.actor(x))\n",
    "        \n",
    "\n",
    "        action_prob = F.softmax(action_scores, dim=-1)\n",
    "\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        state_values = self.critic(x)\n",
    "        # state_values = F.softmax(state_values)\n",
    "        # print(f\"{state_values= }\")\n",
    "        \n",
    "        \n",
    "        return action_prob, state_values\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664b08b",
   "metadata": {},
   "source": [
    "# <font color='indianred'><b> New Model Network </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca062e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic_new, self).__init__()\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(23, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512,1)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(23, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 60),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        state_values = self.critic(x)\n",
    "        action_probs = self.actor(x)\n",
    "\n",
    "        return action_probs, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6191b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_size\n",
    "state_space_size = env.obs.shape[0]\n",
    "\n",
    "# Lists for plotting\n",
    "episode_list = []\n",
    "reward_list = []\n",
    "rwrd_list = []\n",
    "loss_list = []\n",
    "policy_loss_list = []\n",
    "value_loss_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfefd60",
   "metadata": {},
   "source": [
    "# <font color='violet'><b> Specifications </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e24b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-7\n",
    "\n",
    "model = ActorCritic()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Adaptive Learning Rate\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff555b9",
   "metadata": {},
   "source": [
    "# <font color='crimson'><b> Action Selection </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model.forward(state)\n",
    "\n",
    "    # flag = random()\n",
    "  \n",
    "    # exploration_rate_threshold = random.uniform(0,1)\n",
    "    # print(len(probs), len(state_value))\n",
    "\n",
    "    # print(state)\n",
    "    # print(probs)\n",
    "\n",
    "    # probs = math.log(probs)\n",
    "    logging.info('state:' + str(state))\n",
    "    logging.info('state_value:' +str(state_value))\n",
    "    logging.info('probs:' + str(probs))\n",
    "    m = Categorical(probs)\n",
    " \n",
    "    logging.info(str(m))\n",
    "    action = m.sample()\n",
    "\n",
    "    logging.info('action:' + str(action))\n",
    "    # print(\"Action: \" , action)   \n",
    "\n",
    "\n",
    "    # else:\n",
    "    # while action.item() != 1 and action.item() != 3 and action.item() != 5\\\n",
    "    #         and action.item() != 7 and action.item() != 12 and action.item() != 14 \\\n",
    "    #              and action.item() != 21 and action.item() != 26 and action.item() != 27 :\n",
    "    #     print(action.item())\n",
    "    #     action = torch.tensor(env.sample_action())\n",
    "        # print(action.item())\n",
    "\n",
    "\n",
    "    # perform one action and assess the results\n",
    "\n",
    "    # action = torch.tensor(env.sample_action())\n",
    "    # print(action)\n",
    "    \n",
    "    # print(\"Chosen Action: \", action.item())\n",
    "\n",
    "\n",
    "    # print(action.item())\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    \n",
    "    return action.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978ae78",
   "metadata": {},
   "source": [
    "# <font color='scarlet'><b> Finish Episode condition</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc36eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import entry_points\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \n",
    "    # We calculate the losses and perform backprop in this function\n",
    "    R = 0\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    #entropy_losses = []\n",
    "    beta = 1e-3\n",
    "    saved_actions = model.saved_actions\n",
    "    returns = []\n",
    "    gamma = 0.99 #Discount factor for past rewards\n",
    "    \n",
    "\n",
    "    # Calculate expected value from rewards\n",
    "    # - At each timestep what's the total reward received after that timestep\n",
    "    # - Rewards in the past are discounted by multiplying them with gamma\n",
    "    # - These are the labels for our critic\n",
    "    # - R: Past Rewards\n",
    "    # - r: total Reward\n",
    "\n",
    "\n",
    "    logging.info(\"len(model.rewards)): \" + str(len(model.rewards)))\n",
    "\n",
    "    logging.info(model.rewards)\n",
    "\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + gamma * R \n",
    "        returns.insert(0, R)\n",
    "        # print(returns)\n",
    "\n",
    "\n",
    "    # R é return real\n",
    "    # value é o retorno do crítico\n",
    "\n",
    "\n",
    "    if len(returns) <= 1:\n",
    "        returns = torch.tensor([0.0])\n",
    "        \n",
    "    # Normalize\n",
    "    else: \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    # print(\"model rewards\" , model.rewards)\n",
    "    # print(\"Returns \" , returns)\n",
    "    # print(\"Saved action\" ,saved_actions)\n",
    "\n",
    "\n",
    "    # Critic train\n",
    "\n",
    "    logging.info(\"len(saved_actions)): \" + str(len(saved_actions)))\n",
    "    logging.info(\"len(returns)): \" + str(len(returns)))\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        # At this point in history, the critic estimated that we would get a\n",
    "        # total reward = `value` in the future. We took an action with log probability\n",
    "        # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "        # The actor must be updated so that it predicts an action that leads to\n",
    "        # high rewards (compared to critic's estimate) with high probability.\n",
    "        advantage = R - value.item()\n",
    "        # print(\"advantage: \", advantage)\n",
    "        \n",
    "       # entropy_loss = -beta * (action_probs * log_prob).sum(dim=1).mean()   \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        # Actor Loss\n",
    "        policy_loss_list.append(policy_losses)\n",
    "        \n",
    "\n",
    "        # The critic must be updated so that it predicts a \n",
    "        # better estimate of future rewards.\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "        value_loss_list.append(value_losses)\n",
    "        \n",
    "        logging.info(\"log_prob: \" + str(log_prob))\n",
    "        \n",
    "        logging.info(\"R: \" + str(R))\n",
    "        logging.info(\"value: \" + str(value))\n",
    "      \n",
    "\n",
    "        \n",
    "        # print(\"Log_prob: \", log_prob)\n",
    "\n",
    "\n",
    "    logging.info(value_losses)\n",
    "    logging.info(policy_losses)\n",
    "    # Sets the gradients to zero before performing backpropagation because pytorch accumulates the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # BackPropagation\n",
    "    # loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # print(\"Policy loss: \", policy_losses)\n",
    "    # print(\"Value loss: \", value_losses)\n",
    "    # print(\"Total Loss\", loss)\n",
    "    loss_list.append(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm= 2)\n",
    "    \n",
    "    # print(\"----------------------------------------------------------GRADIENTS-----------------------------------------------\\n\")\n",
    "    # for p in model.parameters():\n",
    "        #print(p.grad.norm())\n",
    "        #print(loss)\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # print(\"LR = \", scheduler.get_last_lr())\n",
    "\n",
    "    \n",
    "    # Clear the loss and reward history\n",
    "    # del model.rewards[:]\n",
    "    # del model.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6ac10",
   "metadata": {},
   "source": [
    "# <font color='darkorange'><b> Plots </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Colors for plots\n",
    "\n",
    "Colors = ['mediumaquamarine', 'blueviolet', 'lightcoral',\n",
    "            'darkorange', 'royalblue', 'crimson', 'dimgray',\n",
    "            'navy', 'violet', 'teal', 'hotpink', 'peru']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(episode_list, reward_list, intervals=None):\n",
    "  \n",
    "    n_ep = len(episode_list)\n",
    "\n",
    "    if intervals is None:\n",
    "        intervals = [[0, n_ep]]\n",
    "\n",
    "    for i, [l, r] in enumerate(intervals):\n",
    "        plt.plot(episode_list[max(0,l):min(r,n_ep)], reward_list[max(0,l):min(r,n_ep)], color=Colors[i])\n",
    "        print(l, r)\n",
    "    \n",
    "    plt.xlabel('x - # Episode')\n",
    "    plt.ylabel('y - Reward')\n",
    "    plt.title('Reward over episodes')\n",
    "    # plt.xscale('log')\n",
    "    # plt.xscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(episode_list, loss_list):\n",
    "\n",
    "    tp.plot(episode_list, loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Total Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca915fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_losses(episode_list, policy_loss_list):\n",
    "\n",
    "    tp.plot(episode_list, policy_loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Policy Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_losses(episode_list, value_loss_list):\n",
    "\n",
    "    tp.plot(episode_list, value_loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Value Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_last_reward(episode_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28d4b1",
   "metadata": {},
   "source": [
    "# <font color='gold'><b> Training </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = -1\n",
    "    episode_list = []\n",
    "    reward_list = []\n",
    "    t_max = 50\n",
    "        # cnt_up = 0\n",
    "\n",
    "\n",
    "    for i_episode in count(): # We need around this much episodes\n",
    "        state = env.reset(env.values_init)\n",
    "        ep_reward = 0\n",
    "        reward_improvement = 0 \n",
    "        # print(state)\n",
    "        # print(probs)\n",
    "\n",
    "       \n",
    "    \n",
    "        for t in range(1, t_max):\n",
    "            # print(t, end=', \\n')\n",
    "        \n",
    "            action = select_action(state)                   # Action is selected\n",
    "            state, reward, done, log = env.step(action)     # State is updated accordingly\n",
    "\n",
    "            # print(\"reward: \",reward)\n",
    "            # print(\"action: \",action)\n",
    "            # print(\"state: \" ,state)\n",
    "            \n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            # print(model.rewards)\n",
    "            # if i_episode > 0:\n",
    "            #     if model.rewards[t-2] < model.rewards[t-1]:\n",
    "            #             reward_improvement = model.rewards[t-1] - model.rewards[t-2]\n",
    "            #             cnt_up += 1\n",
    "            #             print(\"# Action:\",action, \"Improvement:\", reward_improvement)\n",
    "\n",
    "                # elif model.rewards[t-2] > model.rewards[t-1]:\n",
    "                #     reward_improvement = model.rewards[t-1] - model.rewards[t-2]\n",
    "                #     print(\"# Action:\",action, \"Piorou:\", reward_improvement)\n",
    "\n",
    "            # se nao houver improvement - fazer unstep\n",
    "\n",
    "            ep_reward = reward\n",
    "        \n",
    "\n",
    "            \n",
    "            if done:\n",
    "                print(t, end=', \\n')\n",
    "                break        \n",
    "        \n",
    "        # print(\"nºde melhoramentos: \", cnt_up )\n",
    "\n",
    "        # Breaking conditions in case it does not find a solution that fits the requirements\n",
    "        # if i_episode == 200:\n",
    "        #     break\n",
    "\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "\n",
    "\n",
    "        # Additional lists for plotting\n",
    "        episode_list.append(i_episode)\n",
    "        rwrd_list.append(reward)\n",
    "        reward_list.append(ep_reward) \n",
    "\n",
    "\n",
    "        finish_episode()\n",
    "\n",
    "\n",
    "        # print(\"learning rate = \", learning_rate)\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        del model.rewards[:]\n",
    "        del model.saved_actions[:]\n",
    "\n",
    "\n",
    "        # exploration_rate = min_exploration_rate + \\\n",
    "            # (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*i_episode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % 1 == 0: #% 2 == 0: # We will print some things out\n",
    "            print(\"------------------------Episode {}\\tLast Reward: {:.3f}\\tRunning reward: {:.3f}---------------------------------------------\\n\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            ))\n",
    "            \n",
    "        if running_reward >= 0:\n",
    "            print(\"Solved, running reward is now {} and the last episode runs to {} time steps\".format(\n",
    "                    running_reward, t\n",
    "            ))\n",
    "            print(\"log: \", log)\n",
    "\n",
    "            break\n",
    "           \n",
    "    # plot_reward(episode_list, reward_list, [[0,100],[99,300],[299,500],[499,1000],[999,1500],[1499,3000],[2999,4000],[3999,5000],[4999,6000]])\n",
    "    # # plot_reward()\n",
    "    # plot_losses(episode_list, loss_list)\n",
    "    # plot_policy_losses(episode_list, policy_loss_list)\n",
    "    # plot_value_losses(episode_list, value_loss_list)\n",
    "\n",
    "        \n",
    "    # plot_value_losses(episode_list, value_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81393297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0895c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c75de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6100530",
   "metadata": {},
   "outputs": [],
   "source": [
    "env._run_simulation()\n",
    "env.measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b61689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There. we finished\n",
    "# Lets see it in action\n",
    "done = False\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0338c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_performance, done, log = env.target.verify(env.measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"next performance\", next_performance)\n",
    "print(\"done\", done)\n",
    "print(\"log\", log)\n",
    "env.measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb57d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation = env.reset()\n",
    "# while True:\n",
    "# cnt += 1\n",
    "env.render()\n",
    "# action = select_action(observation)\n",
    "# observation, reward, done, _ = env.step(action)\n",
    "# Lets see how long it lasts until failing\n",
    "# print(f\"Game lasted {cnt} moves\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.py38env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcbf952eab33b6c92f6651d3075f2149c3c129367d1471ae90b0b5f7c391bc5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
