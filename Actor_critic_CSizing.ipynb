{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d412b30e",
   "metadata": {},
   "source": [
    "# <font color='deeppink'><b> Actor-Critic in Reinforcement Learning </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726f6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DeprecationWarning: invalid escape sequence \\m\n",
      "<string>:2: DeprecationWarning: invalid escape sequence \\ \n",
      "<string>:2: DeprecationWarning: invalid escape sequence \\l\n",
      "<string>:2: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20a224ef7f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import random\n",
    "import gym\n",
    "import time \n",
    "import numpy as np\n",
    "import torchplot as tp\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from circuit.vcamp import VCAmpRLEnvDiscrete\n",
    "import math\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Plots \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting a fixed seed for reproducible results\n",
    "np.random.seed(seed=21)\n",
    "torch.manual_seed(seed=21)\n",
    "\n",
    "\n",
    "#import pygame\n",
    "#from pygame import gfxdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60469542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomas\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = VCAmpRLEnvDiscrete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721906e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66defdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 actions\n",
      "(23,)\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} actions\".format(env.action_size))\n",
    "print(env.reset().shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f59ec",
   "metadata": {},
   "source": [
    "# <font color='orangered'><b> Model Network </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5023d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network: Actor-Critic \n",
    "#     \n",
    "# First fully connected layer with 23 inputs (states) and 256 outputs (arbitrary number)\n",
    "# \n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.common1 = torch.nn.Linear(23, 1024) # x because there are x parameters as the observation space\n",
    "        self.common2 = torch.nn.Linear(1024, 512)\n",
    "        \n",
    "        self.actor = torch.nn.Linear(512, 60) # 60 for the number of actions\n",
    "\n",
    "        # self.fc3 = torch.nn.Linear(128,20)\n",
    "        self.critic = torch.nn.Linear(512, 1) # Critic is always 1\n",
    "        \n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "                #print(f\"{x= }\")\n",
    "        #print(f\"{action_scores= }\")\n",
    "\n",
    "        x = F.leaky_relu(self.common1(x))\n",
    "        x = F.leaky_relu(self.common2(x))        \n",
    "\n",
    "        action_scores = torch.sigmoid(self.actor(x))\n",
    "        \n",
    "\n",
    "        action_prob = F.softmax(action_scores, dim=-1)\n",
    "\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        state_values = self.critic(x)\n",
    "        # state_values = F.softmax(state_values)\n",
    "        # print(f\"{state_values= }\")\n",
    "        \n",
    "        \n",
    "        return action_prob, state_values\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664b08b",
   "metadata": {},
   "source": [
    "# <font color='indianred'><b> New Model Network </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca062e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic_new, self).__init__()\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(23, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512,1)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(23, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 60),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        state_values = self.critic(x)\n",
    "        action_probs = self.actor(x)\n",
    "\n",
    "        return action_probs, state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6191b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_size\n",
    "state_space_size = env.obs.shape[0]\n",
    "\n",
    "# Lists for plotting\n",
    "episode_list = []\n",
    "reward_list = []\n",
    "rwrd_list = []\n",
    "loss_list = []\n",
    "policy_loss_list = []\n",
    "value_loss_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfefd60",
   "metadata": {},
   "source": [
    "# <font color='violet'><b> Specifications </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e24b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "\n",
    "model = ActorCritic()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Adaptive Learning Rate\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.2)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff555b9",
   "metadata": {},
   "source": [
    "# <font color='crimson'><b> Action Selection </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d148f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model.forward(state)\n",
    "\n",
    "    # flag = random()\n",
    "  \n",
    "    # exploration_rate_threshold = random.uniform(0,1)\n",
    "    # print(len(probs), len(state_value))\n",
    "\n",
    "    # print(state)\n",
    "    # print(probs)\n",
    "\n",
    "    # probs = math.log(probs)\n",
    "\n",
    "    m = Categorical(probs)\n",
    " \n",
    "    action = m.sample()\n",
    "\n",
    "    # print(\"Action: \" , action)   \n",
    "\n",
    "\n",
    "    # else:\n",
    "    # while action.item() != 1 and action.item() != 3 and action.item() != 5\\\n",
    "    #         and action.item() != 7 and action.item() != 12 and action.item() != 14 \\\n",
    "    #              and action.item() != 21 and action.item() != 26 and action.item() != 27 :\n",
    "    #     print(action.item())\n",
    "    #     action = torch.tensor(env.sample_action())\n",
    "        # print(action.item())\n",
    "\n",
    "\n",
    "    # perform one action and assess the results\n",
    "\n",
    "    # action = torch.tensor(env.sample_action())\n",
    "    # print(action)\n",
    "    \n",
    "    # print(\"Chosen Action: \", action.item())\n",
    "\n",
    "\n",
    "    # print(action.item())\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    \n",
    "    return action.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978ae78",
   "metadata": {},
   "source": [
    "# <font color='scarlet'><b> Finish Episode condition</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc36eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import entry_points\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \n",
    "    # We calculate the losses and perform backprop in this function\n",
    "    R = 0\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    #entropy_losses = []\n",
    "    beta = 1e-3\n",
    "    saved_actions = model.saved_actions\n",
    "    returns = []\n",
    "    gamma = 0.99 #Discount factor for past rewards\n",
    "    \n",
    "\n",
    "    # Calculate expected value from rewards\n",
    "    # - At each timestep what's the total reward received after that timestep\n",
    "    # - Rewards in the past are discounted by multiplying them with gamma\n",
    "    # - These are the labels for our critic\n",
    "    # - R: Past Rewards\n",
    "    # - r: total Reward\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + gamma * R \n",
    "        returns.insert(0, R)\n",
    "        # print(returns)\n",
    "\n",
    "\n",
    "    # R é return real\n",
    "    # value é o retorno do crítico\n",
    "\n",
    "\n",
    "    if len(returns) <= 1:\n",
    "        returns = torch.tensor([0.0])\n",
    "        \n",
    "    # Normalize\n",
    "    else: \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    # print(\"model rewards\" , model.rewards)\n",
    "    # print(\"Returns \" , returns)\n",
    "    # print(\"Saved action\" ,saved_actions)\n",
    "\n",
    "\n",
    "    # Critic train\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        # At this point in history, the critic estimated that we would get a\n",
    "        # total reward = `value` in the future. We took an action with log probability\n",
    "        # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "        # The actor must be updated so that it predicts an action that leads to\n",
    "        # high rewards (compared to critic's estimate) with high probability.\n",
    "        advantage = R - value.item()\n",
    "        # print(\"advantage: \", advantage)\n",
    "        \n",
    "       # entropy_loss = -beta * (action_probs * log_prob).sum(dim=1).mean()   \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        # Actor Loss\n",
    "        policy_loss_list.append(policy_losses)\n",
    "        \n",
    "\n",
    "        # The critic must be updated so that it predicts a \n",
    "        # better estimate of future rewards.\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "        value_loss_list.append(value_losses)\n",
    "        # print(\"Log_prob: \", log_prob)\n",
    "\n",
    "\n",
    "    # Sets the gradients to zero before performing backpropagation because pytorch accumulates the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # BackPropagation\n",
    "    # loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # print(\"Policy loss: \", policy_losses)\n",
    "    # print(\"Value loss: \", value_losses)\n",
    "    # print(\"Total Loss\", loss)\n",
    "    loss_list.append(loss)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm= 1)\n",
    "    \n",
    "    # print(\"----------------------------------------------------------GRADIENTS-----------------------------------------------\\n\")\n",
    "    # for p in model.parameters():\n",
    "        #print(p.grad.norm())\n",
    "        #print(loss)\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # print(\"LR = \", scheduler.get_last_lr())\n",
    "\n",
    "    \n",
    "    # Clear the loss and reward history\n",
    "    # del model.rewards[:]\n",
    "    # del model.saved_actions[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6ac10",
   "metadata": {},
   "source": [
    "# <font color='darkorange'><b> Plots </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Colors for plots\n",
    "\n",
    "Colors = ['mediumaquamarine', 'blueviolet', 'lightcoral',\n",
    "            'darkorange', 'royalblue', 'crimson', 'dimgray',\n",
    "            'navy', 'violet', 'teal', 'hotpink', 'peru']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ba2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(episode_list, reward_list, intervals=None):\n",
    "  \n",
    "    n_ep = len(episode_list)\n",
    "\n",
    "    if intervals is None:\n",
    "        intervals = [[0, n_ep]]\n",
    "\n",
    "    for i, [l, r] in enumerate(intervals):\n",
    "        plt.plot(episode_list[max(0,l):min(r,n_ep)], reward_list[max(0,l):min(r,n_ep)], color=Colors[i])\n",
    "        print(l, r)\n",
    "    \n",
    "    plt.xlabel('x - # Episode')\n",
    "    plt.ylabel('y - Reward')\n",
    "    plt.title('Reward over episodes')\n",
    "    # plt.xscale('log')\n",
    "    # plt.xscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a27f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(episode_list, loss_list):\n",
    "\n",
    "    tp.plot(episode_list, loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Total Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fca915fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_losses(episode_list, policy_loss_list):\n",
    "\n",
    "    tp.plot(episode_list, policy_loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Policy Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "888f51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_losses(episode_list, value_loss_list):\n",
    "\n",
    "    tp.plot(episode_list, value_loss_list)\n",
    "    tp.xlabel('x - # Episode')\n",
    "    tp.ylabel('y - Loss')\n",
    "    tp.title('Value Losses over episodes')\n",
    "    tp.grid()\n",
    "    tp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f0d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_last_reward(episode_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28d4b1",
   "metadata": {},
   "source": [
    "# <font color='gold'><b> Training </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2e10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = -1\n",
    "    episode_list = []\n",
    "    reward_list = []\n",
    "    t_max = 50\n",
    "        # cnt_up = 0\n",
    "\n",
    "\n",
    "    for i_episode in count(): # We need around this much episodes\n",
    "        state = env.reset(env.values_init)\n",
    "        ep_reward = 0\n",
    "        reward_improvement = 0 \n",
    "        # print(state)\n",
    "        # print(probs)\n",
    "\n",
    "       \n",
    "    \n",
    "        for t in range(1, t_max):\n",
    "            # print(t, end=', \\n')\n",
    "        \n",
    "            action = select_action(state)                   # Action is selected\n",
    "            state, reward, done, log = env.step(action)     # State is updated accordingly\n",
    "\n",
    "            # print(\"reward: \",reward)\n",
    "            # print(\"action: \",action)\n",
    "            \n",
    "\n",
    "            # model.rewards.append(reward)\n",
    "            # print(model.rewards)\n",
    "            # if i_episode > 0:\n",
    "            #     if model.rewards[t-2] < model.rewards[t-1]:\n",
    "            #             reward_improvement = model.rewards[t-1] - model.rewards[t-2]\n",
    "            #             cnt_up += 1\n",
    "            #             print(\"# Action:\",action, \"Improvement:\", reward_improvement)\n",
    "\n",
    "                # elif model.rewards[t-2] > model.rewards[t-1]:\n",
    "                #     reward_improvement = model.rewards[t-1] - model.rewards[t-2]\n",
    "                #     print(\"# Action:\",action, \"Piorou:\", reward_improvement)\n",
    "\n",
    "            # se nao houver improvement - fazer unstep\n",
    "\n",
    "            ep_reward = reward\n",
    "        \n",
    "\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # print(\"nºde melhoramentos: \", cnt_up )\n",
    "\n",
    "        # Breaking conditions in case it does not find a solution that fits the requirements\n",
    "        # if i_episode == 200:\n",
    "        #     break\n",
    "\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "\n",
    "\n",
    "        # Additional lists for plotting\n",
    "        episode_list.append(i_episode)\n",
    "        rwrd_list.append(reward)\n",
    "        reward_list.append(ep_reward) \n",
    "\n",
    "\n",
    "        finish_episode()\n",
    "\n",
    "\n",
    "        # print(\"learning rate = \", learning_rate)\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        # finish_episode()\n",
    "        del model.rewards[:]\n",
    "        del model.saved_actions[:]\n",
    "\n",
    "\n",
    "        # exploration_rate = min_exploration_rate + \\\n",
    "            # (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*i_episode)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % 1 == 0: #% 2 == 0: # We will print some things out\n",
    "            print(\"------------------------Episode {}\\tLast Reward: {:.3f}\\tRunning reward: {:.3f}---------------------------------------------\\n\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            ))\n",
    "            \n",
    "        if running_reward >= 0:\n",
    "            print(\"Solved, running reward is now {} and the last episode runs to {} time steps\".format(\n",
    "                    running_reward, t\n",
    "            ))\n",
    "            print(\"log: \", log)\n",
    "\n",
    "            break\n",
    "           \n",
    "    # plot_reward(episode_list, reward_list, [[0,100],[99,300],[299,500],[499,1000],[999,1500],[1499,3000],[2999,4000],[3999,5000],[4999,6000]])\n",
    "    # # plot_reward()\n",
    "    # plot_losses(episode_list, loss_list)\n",
    "    # plot_policy_losses(episode_list, policy_loss_list)\n",
    "    # plot_value_losses(episode_list, value_loss_list)\n",
    "\n",
    "        \n",
    "    # plot_value_losses(episode_list, value_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81393297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------Episode 0\tLast Reward: -100.000\tRunning reward: -5.950---------------------------------------------\n",
      "\n",
      "------------------------Episode 1\tLast Reward: -4.962\tRunning reward: -5.901---------------------------------------------\n",
      "\n",
      "------------------------Episode 2\tLast Reward: -0.704\tRunning reward: -5.641---------------------------------------------\n",
      "\n",
      "------------------------Episode 3\tLast Reward: -9.078\tRunning reward: -5.813---------------------------------------------\n",
      "\n",
      "------------------------Episode 4\tLast Reward: -2.696\tRunning reward: -5.657---------------------------------------------\n",
      "\n",
      "------------------------Episode 5\tLast Reward: -20.254\tRunning reward: -6.387---------------------------------------------\n",
      "\n",
      "------------------------Episode 6\tLast Reward: -100.000\tRunning reward: -11.067---------------------------------------------\n",
      "\n",
      "------------------------Episode 7\tLast Reward: -3.890\tRunning reward: -10.708---------------------------------------------\n",
      "\n",
      "------------------------Episode 8\tLast Reward: -100.000\tRunning reward: -15.173---------------------------------------------\n",
      "\n",
      "------------------------Episode 9\tLast Reward: -7.478\tRunning reward: -14.788---------------------------------------------\n",
      "\n",
      "------------------------Episode 10\tLast Reward: -100.000\tRunning reward: -19.049---------------------------------------------\n",
      "\n",
      "------------------------Episode 11\tLast Reward: -13.833\tRunning reward: -18.788---------------------------------------------\n",
      "\n",
      "------------------------Episode 12\tLast Reward: -39.877\tRunning reward: -19.843---------------------------------------------\n",
      "\n",
      "------------------------Episode 13\tLast Reward: -100.000\tRunning reward: -23.850---------------------------------------------\n",
      "\n",
      "------------------------Episode 14\tLast Reward: -100.000\tRunning reward: -27.658---------------------------------------------\n",
      "\n",
      "------------------------Episode 15\tLast Reward: -100.000\tRunning reward: -31.275---------------------------------------------\n",
      "\n",
      "------------------------Episode 16\tLast Reward: -7.319\tRunning reward: -30.077---------------------------------------------\n",
      "\n",
      "------------------------Episode 17\tLast Reward: -100.000\tRunning reward: -33.573---------------------------------------------\n",
      "\n",
      "------------------------Episode 18\tLast Reward: -100.000\tRunning reward: -36.895---------------------------------------------\n",
      "\n",
      "------------------------Episode 19\tLast Reward: -2.708\tRunning reward: -35.185---------------------------------------------\n",
      "\n",
      "------------------------Episode 20\tLast Reward: -1.746\tRunning reward: -33.513---------------------------------------------\n",
      "\n",
      "------------------------Episode 21\tLast Reward: -100.000\tRunning reward: -36.838---------------------------------------------\n",
      "\n",
      "------------------------Episode 22\tLast Reward: -8.044\tRunning reward: -35.398---------------------------------------------\n",
      "\n",
      "------------------------Episode 23\tLast Reward: -100.000\tRunning reward: -38.628---------------------------------------------\n",
      "\n",
      "------------------------Episode 24\tLast Reward: -0.247\tRunning reward: -36.709---------------------------------------------\n",
      "\n",
      "------------------------Episode 25\tLast Reward: -3.009\tRunning reward: -35.024---------------------------------------------\n",
      "\n",
      "------------------------Episode 26\tLast Reward: -6.440\tRunning reward: -33.595---------------------------------------------\n",
      "\n",
      "------------------------Episode 27\tLast Reward: -3.403\tRunning reward: -32.085---------------------------------------------\n",
      "\n",
      "------------------------Episode 28\tLast Reward: -100.000\tRunning reward: -35.481---------------------------------------------\n",
      "\n",
      "------------------------Episode 29\tLast Reward: -84.869\tRunning reward: -37.950---------------------------------------------\n",
      "\n",
      "------------------------Episode 30\tLast Reward: -6.363\tRunning reward: -36.371---------------------------------------------\n",
      "\n",
      "------------------------Episode 31\tLast Reward: -100.000\tRunning reward: -39.553---------------------------------------------\n",
      "\n",
      "------------------------Episode 32\tLast Reward: -100.000\tRunning reward: -42.575---------------------------------------------\n",
      "\n",
      "------------------------Episode 33\tLast Reward: -7.000\tRunning reward: -40.796---------------------------------------------\n",
      "\n",
      "------------------------Episode 34\tLast Reward: -3.835\tRunning reward: -38.948---------------------------------------------\n",
      "\n",
      "------------------------Episode 35\tLast Reward: -7.063\tRunning reward: -37.354---------------------------------------------\n",
      "\n",
      "------------------------Episode 36\tLast Reward: -6.372\tRunning reward: -35.805---------------------------------------------\n",
      "\n",
      "------------------------Episode 37\tLast Reward: -100.000\tRunning reward: -39.015---------------------------------------------\n",
      "\n",
      "------------------------Episode 38\tLast Reward: -100.000\tRunning reward: -42.064---------------------------------------------\n",
      "\n",
      "------------------------Episode 39\tLast Reward: -2.471\tRunning reward: -40.084---------------------------------------------\n",
      "\n",
      "------------------------Episode 40\tLast Reward: -2.005\tRunning reward: -38.180---------------------------------------------\n",
      "\n",
      "------------------------Episode 41\tLast Reward: -1.918\tRunning reward: -36.367---------------------------------------------\n",
      "\n",
      "------------------------Episode 42\tLast Reward: -100.000\tRunning reward: -39.549---------------------------------------------\n",
      "\n",
      "------------------------Episode 43\tLast Reward: -100.000\tRunning reward: -42.571---------------------------------------------\n",
      "\n",
      "------------------------Episode 44\tLast Reward: -3.595\tRunning reward: -40.623---------------------------------------------\n",
      "\n",
      "------------------------Episode 45\tLast Reward: -5.716\tRunning reward: -38.877---------------------------------------------\n",
      "\n",
      "------------------------Episode 46\tLast Reward: -3.730\tRunning reward: -37.120---------------------------------------------\n",
      "\n",
      "------------------------Episode 47\tLast Reward: -100.000\tRunning reward: -40.264---------------------------------------------\n",
      "\n",
      "------------------------Episode 48\tLast Reward: -100.000\tRunning reward: -43.251---------------------------------------------\n",
      "\n",
      "------------------------Episode 49\tLast Reward: -100.000\tRunning reward: -46.088---------------------------------------------\n",
      "\n",
      "------------------------Episode 50\tLast Reward: -100.000\tRunning reward: -48.784---------------------------------------------\n",
      "\n",
      "------------------------Episode 51\tLast Reward: -100.000\tRunning reward: -51.345---------------------------------------------\n",
      "\n",
      "------------------------Episode 52\tLast Reward: -4.457\tRunning reward: -49.000---------------------------------------------\n",
      "\n",
      "------------------------Episode 53\tLast Reward: -46.156\tRunning reward: -48.858---------------------------------------------\n",
      "\n",
      "------------------------Episode 54\tLast Reward: -100.000\tRunning reward: -51.415---------------------------------------------\n",
      "\n",
      "------------------------Episode 55\tLast Reward: -100.000\tRunning reward: -53.844---------------------------------------------\n",
      "\n",
      "------------------------Episode 56\tLast Reward: -100.000\tRunning reward: -56.152---------------------------------------------\n",
      "\n",
      "------------------------Episode 57\tLast Reward: -1.543\tRunning reward: -53.422---------------------------------------------\n",
      "\n",
      "------------------------Episode 58\tLast Reward: -3.145\tRunning reward: -50.908---------------------------------------------\n",
      "\n",
      "------------------------Episode 59\tLast Reward: -3.408\tRunning reward: -48.533---------------------------------------------\n",
      "\n",
      "------------------------Episode 60\tLast Reward: -5.515\tRunning reward: -46.382---------------------------------------------\n",
      "\n",
      "------------------------Episode 61\tLast Reward: -3.128\tRunning reward: -44.219---------------------------------------------\n",
      "\n",
      "------------------------Episode 62\tLast Reward: -8.599\tRunning reward: -42.438---------------------------------------------\n",
      "\n",
      "------------------------Episode 63\tLast Reward: -100.000\tRunning reward: -45.316---------------------------------------------\n",
      "\n",
      "------------------------Episode 64\tLast Reward: -100.000\tRunning reward: -48.051---------------------------------------------\n",
      "\n",
      "------------------------Episode 65\tLast Reward: -2.705\tRunning reward: -45.783---------------------------------------------\n",
      "\n",
      "------------------------Episode 66\tLast Reward: -7.514\tRunning reward: -43.870---------------------------------------------\n",
      "\n",
      "------------------------Episode 67\tLast Reward: -8.318\tRunning reward: -42.092---------------------------------------------\n",
      "\n",
      "------------------------Episode 68\tLast Reward: -100.000\tRunning reward: -44.988---------------------------------------------\n",
      "\n",
      "------------------------Episode 69\tLast Reward: -5.840\tRunning reward: -43.030---------------------------------------------\n",
      "\n",
      "------------------------Episode 70\tLast Reward: -2.816\tRunning reward: -41.019---------------------------------------------\n",
      "\n",
      "------------------------Episode 71\tLast Reward: -7.675\tRunning reward: -39.352---------------------------------------------\n",
      "\n",
      "------------------------Episode 72\tLast Reward: -100.000\tRunning reward: -42.385---------------------------------------------\n",
      "\n",
      "------------------------Episode 73\tLast Reward: -1.086\tRunning reward: -40.320---------------------------------------------\n",
      "\n",
      "------------------------Episode 74\tLast Reward: -100.000\tRunning reward: -43.304---------------------------------------------\n",
      "\n",
      "------------------------Episode 75\tLast Reward: -1.643\tRunning reward: -41.221---------------------------------------------\n",
      "\n",
      "------------------------Episode 76\tLast Reward: -100.000\tRunning reward: -44.160---------------------------------------------\n",
      "\n",
      "------------------------Episode 77\tLast Reward: -3.333\tRunning reward: -42.118---------------------------------------------\n",
      "\n",
      "------------------------Episode 78\tLast Reward: -7.370\tRunning reward: -40.381---------------------------------------------\n",
      "\n",
      "------------------------Episode 79\tLast Reward: -1.998\tRunning reward: -38.462---------------------------------------------\n",
      "\n",
      "------------------------Episode 80\tLast Reward: -100.000\tRunning reward: -41.539---------------------------------------------\n",
      "\n",
      "------------------------Episode 81\tLast Reward: -6.269\tRunning reward: -39.775---------------------------------------------\n",
      "\n",
      "------------------------Episode 82\tLast Reward: -2.119\tRunning reward: -37.892---------------------------------------------\n",
      "\n",
      "------------------------Episode 83\tLast Reward: -100.000\tRunning reward: -40.998---------------------------------------------\n",
      "\n",
      "------------------------Episode 84\tLast Reward: -3.742\tRunning reward: -39.135---------------------------------------------\n",
      "\n",
      "------------------------Episode 85\tLast Reward: -3.222\tRunning reward: -37.339---------------------------------------------\n",
      "\n",
      "------------------------Episode 86\tLast Reward: -2.805\tRunning reward: -35.613---------------------------------------------\n",
      "\n",
      "------------------------Episode 87\tLast Reward: -2.291\tRunning reward: -33.946---------------------------------------------\n",
      "\n",
      "------------------------Episode 88\tLast Reward: -100.000\tRunning reward: -37.249---------------------------------------------\n",
      "\n",
      "------------------------Episode 89\tLast Reward: -100.000\tRunning reward: -40.387---------------------------------------------\n",
      "\n",
      "------------------------Episode 90\tLast Reward: -100.000\tRunning reward: -43.367---------------------------------------------\n",
      "\n",
      "------------------------Episode 91\tLast Reward: -0.726\tRunning reward: -41.235---------------------------------------------\n",
      "\n",
      "------------------------Episode 92\tLast Reward: -100.000\tRunning reward: -44.174---------------------------------------------\n",
      "\n",
      "------------------------Episode 93\tLast Reward: -100.000\tRunning reward: -46.965---------------------------------------------\n",
      "\n",
      "------------------------Episode 94\tLast Reward: -100.000\tRunning reward: -49.617---------------------------------------------\n",
      "\n",
      "------------------------Episode 95\tLast Reward: -100.000\tRunning reward: -52.136---------------------------------------------\n",
      "\n",
      "------------------------Episode 96\tLast Reward: -100.000\tRunning reward: -54.529---------------------------------------------\n",
      "\n",
      "------------------------Episode 97\tLast Reward: -4.373\tRunning reward: -52.021---------------------------------------------\n",
      "\n",
      "------------------------Episode 98\tLast Reward: -100.000\tRunning reward: -54.420---------------------------------------------\n",
      "\n",
      "------------------------Episode 99\tLast Reward: -100.000\tRunning reward: -56.699---------------------------------------------\n",
      "\n",
      "------------------------Episode 100\tLast Reward: -3.297\tRunning reward: -54.029---------------------------------------------\n",
      "\n",
      "------------------------Episode 101\tLast Reward: -4.145\tRunning reward: -51.535---------------------------------------------\n",
      "\n",
      "------------------------Episode 102\tLast Reward: -1.360\tRunning reward: -49.026---------------------------------------------\n",
      "\n",
      "------------------------Episode 103\tLast Reward: -100.000\tRunning reward: -51.575---------------------------------------------\n",
      "\n",
      "------------------------Episode 104\tLast Reward: -100.000\tRunning reward: -53.996---------------------------------------------\n",
      "\n",
      "------------------------Episode 105\tLast Reward: -2.690\tRunning reward: -51.431---------------------------------------------\n",
      "\n",
      "------------------------Episode 106\tLast Reward: -39.199\tRunning reward: -50.819---------------------------------------------\n",
      "\n",
      "------------------------Episode 107\tLast Reward: -8.264\tRunning reward: -48.691---------------------------------------------\n",
      "\n",
      "------------------------Episode 108\tLast Reward: -3.110\tRunning reward: -46.412---------------------------------------------\n",
      "\n",
      "------------------------Episode 109\tLast Reward: -3.842\tRunning reward: -44.284---------------------------------------------\n",
      "\n",
      "------------------------Episode 110\tLast Reward: -2.864\tRunning reward: -42.213---------------------------------------------\n",
      "\n",
      "------------------------Episode 111\tLast Reward: -100.000\tRunning reward: -45.102---------------------------------------------\n",
      "\n",
      "------------------------Episode 112\tLast Reward: -100.000\tRunning reward: -47.847---------------------------------------------\n",
      "\n",
      "------------------------Episode 113\tLast Reward: -2.881\tRunning reward: -45.599---------------------------------------------\n",
      "\n",
      "------------------------Episode 114\tLast Reward: -100.000\tRunning reward: -48.319---------------------------------------------\n",
      "\n",
      "------------------------Episode 115\tLast Reward: -7.256\tRunning reward: -46.266---------------------------------------------\n",
      "\n",
      "------------------------Episode 116\tLast Reward: -12.416\tRunning reward: -44.573---------------------------------------------\n",
      "\n",
      "------------------------Episode 117\tLast Reward: -2.004\tRunning reward: -42.445---------------------------------------------\n",
      "\n",
      "------------------------Episode 118\tLast Reward: -100.000\tRunning reward: -45.323---------------------------------------------\n",
      "\n",
      "------------------------Episode 119\tLast Reward: -7.057\tRunning reward: -43.409---------------------------------------------\n",
      "\n",
      "------------------------Episode 120\tLast Reward: -100.000\tRunning reward: -46.239---------------------------------------------\n",
      "\n",
      "------------------------Episode 121\tLast Reward: -100.000\tRunning reward: -48.927---------------------------------------------\n",
      "\n",
      "------------------------Episode 122\tLast Reward: -2.573\tRunning reward: -46.609---------------------------------------------\n",
      "\n",
      "------------------------Episode 123\tLast Reward: -100.000\tRunning reward: -49.279---------------------------------------------\n",
      "\n",
      "------------------------Episode 124\tLast Reward: -3.196\tRunning reward: -46.975---------------------------------------------\n",
      "\n",
      "------------------------Episode 125\tLast Reward: -2.702\tRunning reward: -44.761---------------------------------------------\n",
      "\n",
      "------------------------Episode 126\tLast Reward: -100.000\tRunning reward: -47.523---------------------------------------------\n",
      "\n",
      "------------------------Episode 127\tLast Reward: -100.000\tRunning reward: -50.147---------------------------------------------\n",
      "\n",
      "------------------------Episode 128\tLast Reward: -100.000\tRunning reward: -52.639---------------------------------------------\n",
      "\n",
      "------------------------Episode 129\tLast Reward: -100.000\tRunning reward: -55.007---------------------------------------------\n",
      "\n",
      "------------------------Episode 130\tLast Reward: -0.601\tRunning reward: -52.287---------------------------------------------\n",
      "\n",
      "------------------------Episode 131\tLast Reward: -100.000\tRunning reward: -54.673---------------------------------------------\n",
      "\n",
      "------------------------Episode 132\tLast Reward: -100.000\tRunning reward: -56.939---------------------------------------------\n",
      "\n",
      "------------------------Episode 133\tLast Reward: -18.387\tRunning reward: -55.012---------------------------------------------\n",
      "\n",
      "------------------------Episode 134\tLast Reward: -100.000\tRunning reward: -57.261---------------------------------------------\n",
      "\n",
      "------------------------Episode 135\tLast Reward: -2.731\tRunning reward: -54.534---------------------------------------------\n",
      "\n",
      "------------------------Episode 136\tLast Reward: -100.000\tRunning reward: -56.808---------------------------------------------\n",
      "\n",
      "------------------------Episode 137\tLast Reward: -141.972\tRunning reward: -61.066---------------------------------------------\n",
      "\n",
      "------------------------Episode 138\tLast Reward: -100.000\tRunning reward: -63.013---------------------------------------------\n",
      "\n",
      "------------------------Episode 139\tLast Reward: -100.000\tRunning reward: -64.862---------------------------------------------\n",
      "\n",
      "------------------------Episode 140\tLast Reward: -100.000\tRunning reward: -66.619---------------------------------------------\n",
      "\n",
      "------------------------Episode 141\tLast Reward: -100.000\tRunning reward: -68.288---------------------------------------------\n",
      "\n",
      "------------------------Episode 142\tLast Reward: -6.371\tRunning reward: -65.192---------------------------------------------\n",
      "\n",
      "------------------------Episode 143\tLast Reward: -100.000\tRunning reward: -66.933---------------------------------------------\n",
      "\n",
      "------------------------Episode 144\tLast Reward: -6.999\tRunning reward: -63.936---------------------------------------------\n",
      "\n",
      "------------------------Episode 145\tLast Reward: -1.265\tRunning reward: -60.802---------------------------------------------\n",
      "\n",
      "------------------------Episode 146\tLast Reward: -100.000\tRunning reward: -62.762---------------------------------------------\n",
      "\n",
      "------------------------Episode 147\tLast Reward: -100.000\tRunning reward: -64.624---------------------------------------------\n",
      "\n",
      "------------------------Episode 148\tLast Reward: -11.196\tRunning reward: -61.953---------------------------------------------\n",
      "\n",
      "------------------------Episode 149\tLast Reward: -7.587\tRunning reward: -59.234---------------------------------------------\n",
      "\n",
      "------------------------Episode 150\tLast Reward: -7.076\tRunning reward: -56.626---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0895c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c75de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6100530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gdc': 45.10923,\n",
       " 'gbw': 4265056.0,\n",
       " 'pm': 86.97923,\n",
       " 'inoise_total': 0.0005206988,\n",
       " 'onoise_total': 0.004807285,\n",
       " 'output': '-3.581352194736986e-05',\n",
       " 'in_p': '1.650000000000000e+00',\n",
       " 'in_n': '1.650000000000000e+00',\n",
       " 'm_xinova_mpm0_vgs': '7.653506270871362e-01',\n",
       " 'm_xinova_mpm0_vth': '4.437641214376704e-01',\n",
       " 'm_xinova_mpm0_vds': '1.818410816440337e+00',\n",
       " 'm_xinova_mpm0_vdsat': '2.986787174477666e-01',\n",
       " 'm_xinova_mpm0_id': '9.045984437892819e-06',\n",
       " 'm_xinova_mpm1_vgs': '7.653506270863142e-01',\n",
       " 'm_xinova_mpm1_vth': '4.467723625563421e-01',\n",
       " 'm_xinova_mpm1_vds': '7.653506270871309e-01',\n",
       " 'm_xinova_mpm1_vdsat': '2.916903717702144e-01',\n",
       " 'm_xinova_mpm1_id': '2.946130449266373e-05',\n",
       " 'm_xinova_mpm2_vgs': '7.653499681166545e-01',\n",
       " 'm_xinova_mpm2_vth': '4.467723625569853e-01',\n",
       " 'm_xinova_mpm2_vds': '7.653499681167788e-01',\n",
       " 'm_xinova_mpm2_vdsat': '2.916898507203639e-01',\n",
       " 'm_xinova_mpm2_id': '2.946119662330257e-05',\n",
       " 'm_xinova_mpm3_vgs': '7.653499681167841e-01',\n",
       " 'm_xinova_mpm3_vth': '4.437640574401617e-01',\n",
       " 'm_xinova_mpm3_vds': '1.818446629962285e+00',\n",
       " 'm_xinova_mpm3_vdsat': '2.986782467889127e-01',\n",
       " 'm_xinova_mpm3_id': '9.045966329197252e-06',\n",
       " 'm_xinova_mnm4_vgs': '4.169449989713898e-01',\n",
       " 'm_xinova_mnm4_vth': '4.931140447336451e-01',\n",
       " 'm_xinova_mnm4_vds': '1.301594371602597e+00',\n",
       " 'm_xinova_mnm4_vdsat': '4.906819733568291e-02',\n",
       " 'm_xinova_mnm4_id': '2.945764255098318e-05',\n",
       " 'm_xinova_mnm5_vgs': '4.169448181751971e-01',\n",
       " 'm_xinova_mnm5_vth': '4.931140440346956e-01',\n",
       " 'm_xinova_mnm5_vds': '1.301594849213428e+00',\n",
       " 'm_xinova_mnm5_vdsat': '4.906816859470231e-02',\n",
       " 'm_xinova_mnm5_id': '2.945753461362877e-05',\n",
       " 'm_xinova_mnm6_vgs': '4.169486127546069e-01',\n",
       " 'm_xinova_mnm6_vth': '4.947949371917058e-01',\n",
       " 'm_xinova_mnm6_vds': '2.066944817330207e+00',\n",
       " 'm_xinova_mnm6_vdsat': '4.846058365745728e-02',\n",
       " 'm_xinova_mnm6_id': '2.059599029978721e-05',\n",
       " 'm_xinova_mnm7_vgs': '4.169483903536619e-01',\n",
       " 'm_xinova_mnm7_vth': '4.947949371914811e-01',\n",
       " 'm_xinova_mnm7_vds': '2.066944998689728e+00',\n",
       " 'm_xinova_mnm7_vdsat': '4.846054844540523e-02',\n",
       " 'm_xinova_mnm7_id': '2.059589616771156e-05',\n",
       " 'm_xinova_mnm8_vgs': '1.649999741734567e+00',\n",
       " 'm_xinova_mnm8_vth': '4.947523659192976e-01',\n",
       " 'm_xinova_mnm8_vds': '1.233055182669794e+00',\n",
       " 'm_xinova_mnm8_vdsat': '8.615660342255308e-01',\n",
       " 'm_xinova_mnm8_id': '5.205389266472150e-05',\n",
       " 'm_xinova_mnm9_vgs': '1.649999940400219e+00',\n",
       " 'm_xinova_mnm9_vth': '4.947523659194274e-01',\n",
       " 'm_xinova_mnm9_vds': '1.233055001310273e+00',\n",
       " 'm_xinova_mnm9_vdsat': '8.615661696136361e-01',\n",
       " 'm_xinova_mnm9_id': '5.205390494030385e-05',\n",
       " 'm_xinova_mnm10_vgs': '4.741579087452015e-01',\n",
       " 'm_xinova_mnm10_vth': '4.948002117996970e-01',\n",
       " 'm_xinova_mnm10_vds': '1.481589183559663e+00',\n",
       " 'm_xinova_mnm10_vdsat': '6.167518125534069e-02',\n",
       " 'm_xinova_mnm10_id': '9.053554274041581e-06',\n",
       " 'm_xinova_mnm11_vgs': '4.741579087451839e-01',\n",
       " 'm_xinova_mnm11_vth': '4.948002123628651e-01',\n",
       " 'm_xinova_mnm11_vds': '1.481553370037716e+00',\n",
       " 'm_xinova_mnm11_vdsat': '6.167518107961757e-02',\n",
       " 'm_xinova_mnm11_id': '9.053542484093677e-06',\n",
       " 'vdc_i': '-2.229272198818734e-04',\n",
       " 'idd': 0.0001229272198818734,\n",
       " 'fom': 208.17469088287336,\n",
       " 'vov_mpm0': 0.3215865056494658,\n",
       " 'vov_mpm1': 0.31857826452997207,\n",
       " 'vov_mpm2': 0.31857760555966913,\n",
       " 'vov_mpm3': 0.32158591067662246,\n",
       " 'vov_mnm4': -0.0761690457622553,\n",
       " 'vov_mnm5': -0.07616922585949853,\n",
       " 'vov_mnm6': -0.07784632443709893,\n",
       " 'vov_mnm7': -0.07784654683781922,\n",
       " 'vov_mnm8': 1.1552473758152695,\n",
       " 'vov_mnm9': 1.1552475744807915,\n",
       " 'vov_mnm10': -0.020642303054495548,\n",
       " 'vov_mnm11': -0.020642303617681212,\n",
       " 'delta_mpm0': 1.5197320989925704,\n",
       " 'delta_mpm1': 0.47366025531691647,\n",
       " 'delta_mpm2': 0.4736601173964149,\n",
       " 'delta_mpm3': 1.5197683831733724,\n",
       " 'delta_mnm4': 1.252526174266914,\n",
       " 'delta_mnm5': 1.2525266806187256,\n",
       " 'delta_mnm6': 2.0184842336727495,\n",
       " 'delta_mnm7': 2.018484450244323,\n",
       " 'delta_mnm8': 0.3714891484442632,\n",
       " 'delta_mnm9': 0.37148883169663693,\n",
       " 'delta_mnm10': 1.4199140023043224,\n",
       " 'delta_mnm11': 1.4198781889580983}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._run_simulation()\n",
    "env.measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b61689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There. we finished\n",
    "# Lets see it in action\n",
    "done = False\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0338c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_performance, done, log = env.target.verify(env.measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec6924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next performance 0\n",
      "done True\n",
      "log {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gdc': 45.10923,\n",
       " 'gbw': 4265056.0,\n",
       " 'pm': 86.97923,\n",
       " 'inoise_total': 0.0005206988,\n",
       " 'onoise_total': 0.004807285,\n",
       " 'output': '-3.581352194736986e-05',\n",
       " 'in_p': '1.650000000000000e+00',\n",
       " 'in_n': '1.650000000000000e+00',\n",
       " 'm_xinova_mpm0_vgs': '7.653506270871362e-01',\n",
       " 'm_xinova_mpm0_vth': '4.437641214376704e-01',\n",
       " 'm_xinova_mpm0_vds': '1.818410816440337e+00',\n",
       " 'm_xinova_mpm0_vdsat': '2.986787174477666e-01',\n",
       " 'm_xinova_mpm0_id': '9.045984437892819e-06',\n",
       " 'm_xinova_mpm1_vgs': '7.653506270863142e-01',\n",
       " 'm_xinova_mpm1_vth': '4.467723625563421e-01',\n",
       " 'm_xinova_mpm1_vds': '7.653506270871309e-01',\n",
       " 'm_xinova_mpm1_vdsat': '2.916903717702144e-01',\n",
       " 'm_xinova_mpm1_id': '2.946130449266373e-05',\n",
       " 'm_xinova_mpm2_vgs': '7.653499681166545e-01',\n",
       " 'm_xinova_mpm2_vth': '4.467723625569853e-01',\n",
       " 'm_xinova_mpm2_vds': '7.653499681167788e-01',\n",
       " 'm_xinova_mpm2_vdsat': '2.916898507203639e-01',\n",
       " 'm_xinova_mpm2_id': '2.946119662330257e-05',\n",
       " 'm_xinova_mpm3_vgs': '7.653499681167841e-01',\n",
       " 'm_xinova_mpm3_vth': '4.437640574401617e-01',\n",
       " 'm_xinova_mpm3_vds': '1.818446629962285e+00',\n",
       " 'm_xinova_mpm3_vdsat': '2.986782467889127e-01',\n",
       " 'm_xinova_mpm3_id': '9.045966329197252e-06',\n",
       " 'm_xinova_mnm4_vgs': '4.169449989713898e-01',\n",
       " 'm_xinova_mnm4_vth': '4.931140447336451e-01',\n",
       " 'm_xinova_mnm4_vds': '1.301594371602597e+00',\n",
       " 'm_xinova_mnm4_vdsat': '4.906819733568291e-02',\n",
       " 'm_xinova_mnm4_id': '2.945764255098318e-05',\n",
       " 'm_xinova_mnm5_vgs': '4.169448181751971e-01',\n",
       " 'm_xinova_mnm5_vth': '4.931140440346956e-01',\n",
       " 'm_xinova_mnm5_vds': '1.301594849213428e+00',\n",
       " 'm_xinova_mnm5_vdsat': '4.906816859470231e-02',\n",
       " 'm_xinova_mnm5_id': '2.945753461362877e-05',\n",
       " 'm_xinova_mnm6_vgs': '4.169486127546069e-01',\n",
       " 'm_xinova_mnm6_vth': '4.947949371917058e-01',\n",
       " 'm_xinova_mnm6_vds': '2.066944817330207e+00',\n",
       " 'm_xinova_mnm6_vdsat': '4.846058365745728e-02',\n",
       " 'm_xinova_mnm6_id': '2.059599029978721e-05',\n",
       " 'm_xinova_mnm7_vgs': '4.169483903536619e-01',\n",
       " 'm_xinova_mnm7_vth': '4.947949371914811e-01',\n",
       " 'm_xinova_mnm7_vds': '2.066944998689728e+00',\n",
       " 'm_xinova_mnm7_vdsat': '4.846054844540523e-02',\n",
       " 'm_xinova_mnm7_id': '2.059589616771156e-05',\n",
       " 'm_xinova_mnm8_vgs': '1.649999741734567e+00',\n",
       " 'm_xinova_mnm8_vth': '4.947523659192976e-01',\n",
       " 'm_xinova_mnm8_vds': '1.233055182669794e+00',\n",
       " 'm_xinova_mnm8_vdsat': '8.615660342255308e-01',\n",
       " 'm_xinova_mnm8_id': '5.205389266472150e-05',\n",
       " 'm_xinova_mnm9_vgs': '1.649999940400219e+00',\n",
       " 'm_xinova_mnm9_vth': '4.947523659194274e-01',\n",
       " 'm_xinova_mnm9_vds': '1.233055001310273e+00',\n",
       " 'm_xinova_mnm9_vdsat': '8.615661696136361e-01',\n",
       " 'm_xinova_mnm9_id': '5.205390494030385e-05',\n",
       " 'm_xinova_mnm10_vgs': '4.741579087452015e-01',\n",
       " 'm_xinova_mnm10_vth': '4.948002117996970e-01',\n",
       " 'm_xinova_mnm10_vds': '1.481589183559663e+00',\n",
       " 'm_xinova_mnm10_vdsat': '6.167518125534069e-02',\n",
       " 'm_xinova_mnm10_id': '9.053554274041581e-06',\n",
       " 'm_xinova_mnm11_vgs': '4.741579087451839e-01',\n",
       " 'm_xinova_mnm11_vth': '4.948002123628651e-01',\n",
       " 'm_xinova_mnm11_vds': '1.481553370037716e+00',\n",
       " 'm_xinova_mnm11_vdsat': '6.167518107961757e-02',\n",
       " 'm_xinova_mnm11_id': '9.053542484093677e-06',\n",
       " 'vdc_i': '-2.229272198818734e-04',\n",
       " 'idd': 0.0001229272198818734,\n",
       " 'fom': 208.17469088287336,\n",
       " 'vov_mpm0': 0.3215865056494658,\n",
       " 'vov_mpm1': 0.31857826452997207,\n",
       " 'vov_mpm2': 0.31857760555966913,\n",
       " 'vov_mpm3': 0.32158591067662246,\n",
       " 'vov_mnm4': -0.0761690457622553,\n",
       " 'vov_mnm5': -0.07616922585949853,\n",
       " 'vov_mnm6': -0.07784632443709893,\n",
       " 'vov_mnm7': -0.07784654683781922,\n",
       " 'vov_mnm8': 1.1552473758152695,\n",
       " 'vov_mnm9': 1.1552475744807915,\n",
       " 'vov_mnm10': -0.020642303054495548,\n",
       " 'vov_mnm11': -0.020642303617681212,\n",
       " 'delta_mpm0': 1.5197320989925704,\n",
       " 'delta_mpm1': 0.47366025531691647,\n",
       " 'delta_mpm2': 0.4736601173964149,\n",
       " 'delta_mpm3': 1.5197683831733724,\n",
       " 'delta_mnm4': 1.252526174266914,\n",
       " 'delta_mnm5': 1.2525266806187256,\n",
       " 'delta_mnm6': 2.0184842336727495,\n",
       " 'delta_mnm7': 2.018484450244323,\n",
       " 'delta_mnm8': 0.3714891484442632,\n",
       " 'delta_mnm9': 0.37148883169663693,\n",
       " 'delta_mnm10': 1.4199140023043224,\n",
       " 'delta_mnm11': 1.4198781889580983}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"next performance\", next_performance)\n",
    "print(\"done\", done)\n",
    "print(\"log\", log)\n",
    "env.measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb57d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values:\n",
      " [-1.9e-05  9.2e-05  3.6e-05  1.2e-05  2.3e-05  0.0e+00  3.7e-06  1.0e-06\n",
      " -2.2e-06  8.9e-07  1.9e-06 -2.0e-07  0.0e+00  2.0e+00  1.0e+01  3.9e+01\n",
      "  3.0e+01  2.1e+01]\n",
      "Performance: 0\n",
      "Eval Log: {'idd_lt': (0.00013, 0.0008436352138416806), 'pm_gt': (45.0, 33.73263)}\n"
     ]
    }
   ],
   "source": [
    "# observation = env.reset()\n",
    "# while True:\n",
    "# cnt += 1\n",
    "env.render()\n",
    "# action = select_action(observation)\n",
    "# observation, reward, done, _ = env.step(action)\n",
    "# Lets see how long it lasts until failing\n",
    "# print(f\"Game lasted {cnt} moves\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e49ad1be67d84e8c05f1355bfb4687abe6018e3beac2c9859fbf3b6d3966e0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
