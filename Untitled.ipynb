{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d412b30e",
   "metadata": {},
   "source": [
    "# <font color='deeppink'><b> Actor-Critic in Reinforcement Learning </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726f6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import pygame\n",
    "from pygame import gfxdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d5b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60469542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0') # We make the Cartpole environment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66defdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 actions\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} actions\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5023d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can move either left or right to balance the pole\n",
    "# Lets implement the Actor critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # 4 because there are 4 parameters as the observation space\n",
    "        self.actor = nn.Linear(128, 2) # 2 for the number of actions\n",
    "        self.critic = nn.Linear(128, 1) # Critic is always 1\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_prob = F.softmax(self.actor(x), dim=-1)\n",
    "        state_values = self.critic(x)\n",
    "        return action_prob, state_values\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d148f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "# In this function, we decide whehter we want the block to move left or right,base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc36eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    # We calculate the losses and perform backprop in this function\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses =[]\n",
    "    returns = []\n",
    "    \n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + 0.99 * R # 0.99 is our gamma number\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73e4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2e10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(): # We need around this much episodes\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0: # We will print some things out\n",
    "            print(\"Episode {}\\tLast Reward: {:.2f}\\tAverage reward: {:.2f}\".format(\n",
    "                i_episode, ep_reward, running_reward\n",
    "            ))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved, running reward is now {} and the last episode runs to {} time steps\".format(\n",
    "                    running_reward, t\n",
    "            ))\n",
    "            break\n",
    "            # This means that we solved cartpole and training is complete\n",
    "                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81393297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast Reward: 19.00\tAverage reward: 10.45\n",
      "Episode 10\tLast Reward: 27.00\tAverage reward: 23.30\n",
      "Episode 20\tLast Reward: 157.00\tAverage reward: 57.70\n",
      "Episode 30\tLast Reward: 81.00\tAverage reward: 65.22\n",
      "Episode 40\tLast Reward: 48.00\tAverage reward: 60.65\n",
      "Episode 50\tLast Reward: 54.00\tAverage reward: 57.67\n",
      "Episode 60\tLast Reward: 93.00\tAverage reward: 57.70\n",
      "Episode 70\tLast Reward: 81.00\tAverage reward: 69.74\n",
      "Episode 80\tLast Reward: 157.00\tAverage reward: 99.83\n",
      "Episode 90\tLast Reward: 21.00\tAverage reward: 127.79\n",
      "Episode 100\tLast Reward: 115.00\tAverage reward: 109.97\n",
      "Episode 110\tLast Reward: 17.00\tAverage reward: 112.54\n",
      "Episode 120\tLast Reward: 136.00\tAverage reward: 93.40\n",
      "Episode 130\tLast Reward: 200.00\tAverage reward: 134.13\n",
      "Episode 140\tLast Reward: 200.00\tAverage reward: 153.57\n",
      "Episode 150\tLast Reward: 200.00\tAverage reward: 172.20\n",
      "Episode 160\tLast Reward: 200.00\tAverage reward: 183.36\n",
      "Episode 170\tLast Reward: 200.00\tAverage reward: 190.03\n",
      "Episode 180\tLast Reward: 81.00\tAverage reward: 178.74\n",
      "Episode 190\tLast Reward: 145.00\tAverage reward: 154.19\n",
      "Episode 200\tLast Reward: 200.00\tAverage reward: 166.23\n",
      "Episode 210\tLast Reward: 200.00\tAverage reward: 179.19\n",
      "Episode 220\tLast Reward: 200.00\tAverage reward: 187.54\n",
      "Episode 230\tLast Reward: 26.00\tAverage reward: 167.94\n",
      "Episode 240\tLast Reward: 200.00\tAverage reward: 173.92\n",
      "Episode 250\tLast Reward: 200.00\tAverage reward: 184.38\n",
      "Episode 260\tLast Reward: 200.00\tAverage reward: 190.65\n",
      "Episode 270\tLast Reward: 200.00\tAverage reward: 194.40\n",
      "Solved, running reward is now 195.2005145401681 and the last episode runs to 200 time steps\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26b61689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There. we finished\n",
    "# Lets see it in action\n",
    "done = False\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb57d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "while True:\n",
    "    cnt += 1\n",
    "    env.render()\n",
    "    action = select_action(observation)\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    # Lets see how long it lasts until failing\n",
    "print(f\"Game lasted {cnt} moves\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
